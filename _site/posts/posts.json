[
  {
    "path": "posts/2021-02-07-making-a-history-table-from-discover-canada-in-r/",
    "title": "Making a History Table of Discover Canada in R",
    "description": "Manipulating text with {quanteda} and creating a table with {reactable}",
    "author": [
      {
        "name": "Umair Durrani",
        "url": {}
      }
    ],
    "date": "2021-02-07",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nCan I use R to create a history timeline?\r\nStep 1: From pdf to dataframe\r\nStep 2: Split the text paragraph in each row into sentences\r\nStep 3: Find the sentences containing years\r\nStep 4: Create a table of year and relevant history:\r\n\r\nDiscover Canada is a study guide that is a required read for anyone preparing for the Canadian citizenship test. It contains information on Canadian government, culture and geography.\r\nThere is also a lot of history sprinkled throughout the text, containing the exact year when something important happened. For example, the current border between Canada and The United States of America was partly a result of the 1812 war between the two countries.\r\n\r\nCan I use R to create a history timeline?\r\nAs a new reader, trying to remember all of the Canadian histoy seems very daunting. I personally prefer a timeline with year and what happened information. So, I initially thought to create something like that by hand, but then:\r\nI recalled that Discover Canada is also available as a pdf and I could potentially import it in R as text\r\nI had heard the term corpus used for text documents in R, and thought that I could maybe convert the imported text into a corpus. A recent R Ladies webinar showed exactly how (Thank you R-Ladies Tunis!)\r\nWith text in R, I could use the tidytext package to extract the sentences and then get the history by years\r\nFinally, I could create a table with history in each year in increasing order using reactable package. My starting point here was Tom Mock’s blog (thanks Tom!)\r\nStep 1: From pdf to dataframe\r\nI started by loading the pdftools and quanteda libraries for reading the Discover Canada pdf file and converting it into a corpus.\r\n\r\n\r\nsuppressPackageStartupMessages(library(pdftools))\r\nsuppressPackageStartupMessages(library(quanteda)) \r\n\r\n\r\n\r\nI first downloaded the large print pdf file from the Discover Canada website. Following shows the page 13 from the pdf:\r\nPage 13 from the Large Print pdf of Discover CanadaNext, I read this file in R:\r\n\r\n\r\npdf_text <- pdf_text(pdf = \"discover-large.pdf\")\r\n\r\nhead(pdf_text, 3)\r\n\r\n\r\n[1] \"             STUDY GUIDE\\r\\n           Discover Canada\\r\\nThe Rights and Responsibilities of Citizenship\\r\\n          LARGE PRINT\\r\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \r\n[2] \"     2\\r\\n\\t\\r  \\r\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \r\n[3] \"The Oath of Citizenship\\r\\nI swear (or affirm)\\r\\nThat I will be faithful\\r\\nAnd bear true allegiance\\r\\nTo Her Majesty Queen Elizabeth the Second\\r\\nQueen of Canada\\r\\nHer Heirs and Successors\\r\\nAnd that I will faithfully observe\\r\\nThe laws of Canada\\r\\nAnd fulfil my duties as a Canadian citizen.\\r\\nLe serment de citoyenneté\\r\\nJe jure (ou j’affirme solennellement)\\r\\nQue je serai fidèle\\r\\nEt porterai sincère allégeance\\r\\nà Sa Majesté la Reine Elizabeth Deux\\r\\nReine du Canada\\r\\nÀ ses héritiers et successeurs\\r\\nQue j’observerai fidèlement les lois du Canada\\r\\nEt que je remplirai loyalement mes obligations\\r\\nde citoyen canadien.\\r\\n                                   3\\r\\n\\t\\r  \\r\\n\" \r\n\r\nThe next step was to convert the raw text into a corpus. This is possible with the corpus() function from the quanteda package:\r\n\r\n\r\ndc_corpus <- corpus(pdf_text)\r\n\r\ndocvars(dc_corpus)\r\n\r\n\r\ndata frame with 0 columns and 129 rows\r\n\r\nThe quanteda::docvars() function lets you find the existing variables and create new variables for the documents in a corpus. There were 129 text “documents” in dc_corpus (you can find it with length(dc_corpus)). We know that there is only 1 document that I am using here, but the pdftools::pdftext() function split the original text into 129 parts. That’s why the corpus now has 129 documents.\r\nInterlude: Tokenization and Visualization of Text Data\r\nNow that the data is living in a corpus, there is a lot that I can do in terms of text analysis. For instance, this awesome post by the R-Ladies presenter, Cosima Meyer introduces the terms for natural language processing and also shows how to clean and visualize text data. Based on that code, I produced a word cloud in three steps below, all powered by the quanteda package:\r\nPre-process the text by removing punctuation, symbols and URL, and splitting the hyphenated words. This pre-processed result is called as tokens.\r\n\r\n\r\n# Text pre-processing\r\ndc_tokens <- tokens(\r\n    # Takes the corpus\r\n    dc_corpus,\r\n    # Remove punctuation\r\n    remove_punct = TRUE,\r\n    # Remove symbols\r\n    remove_symbols = TRUE,\r\n    # Remove URL\r\n    remove_url = TRUE,\r\n    # Split up hyphenated words\r\n    split_hyphens = TRUE\r\n  )\r\n\r\n\r\nhead(dc_tokens, 3)\r\n\r\n\r\nTokens consisting of 3 documents.\r\ntext1 :\r\n [1] \"STUDY\"            \"GUIDE\"            \"Discover\"        \r\n [4] \"Canada\"           \"The\"              \"Rights\"          \r\n [7] \"and\"              \"Responsibilities\" \"of\"              \r\n[10] \"Citizenship\"      \"LARGE\"            \"PRINT\"           \r\n\r\ntext2 :\r\n[1] \"2\"\r\n\r\ntext3 :\r\n [1] \"The\"         \"Oath\"        \"of\"          \"Citizenship\"\r\n [5] \"I\"           \"swear\"       \"or\"          \"affirm\"     \r\n [9] \"That\"        \"I\"           \"will\"        \"be\"         \r\n[ ... and 87 more ]\r\n\r\nCreate a document-feature matrix (DFM). DFM estimates the frequency of each word (‘feature’) across all the text components (‘documents’). While creating DFM you can also stem words. Stemming refers to finding the common root of several words. For example, government and governor have the same root govern. Moreover, you can remove the stopwords that are common words in the language e.g. the, are, etc.\r\n\r\n\r\n# Calculate a document-feature matrix (DFM)\r\n\r\ndc_dfm <- dfm(\r\n  # Take the token object\r\n  dc_tokens,\r\n  # Lower the words\r\n  tolower = TRUE,\r\n  # Get the stem of the words\r\n  stem = TRUE,\r\n  # Remove stop words\r\n  remove = stopwords(\"english\")\r\n)\r\n\r\ndc_dfm\r\n\r\n\r\nDocument-feature matrix of: 129 documents, 3,008 features (97.7% sparse).\r\n       features\r\ndocs    studi guid discov canada right respons citizenship larg print\r\n  text1     1    1      1      1     1       1           1    1     1\r\n  text2     0    0      0      0     0       0           0    0     0\r\n  text3     0    0      0      4     0       0           1    0     0\r\n  text4     0    0      0      5     1       0           1    0     0\r\n  text5     0    0      0      5     2       2           4    0     0\r\n  text6     0    1      0      2     2       1           1    0     0\r\n       features\r\ndocs    2\r\n  text1 0\r\n  text2 1\r\n  text3 0\r\n  text4 0\r\n  text5 0\r\n  text6 0\r\n[ reached max_ndoc ... 123 more documents, reached max_nfeat ... 2,998 more features ]\r\n\r\nCreate a wordcloud:\r\n\r\n\r\nsuppressPackageStartupMessages(library(wesanderson))\r\n\r\ntextplot_wordcloud(\r\n  # Load the DFM object\r\n  dc_dfm,\r\n  # Define the minimum number the words have to occur\r\n  min_count = 3,\r\n  # Define the maximum number the words can occur\r\n  max_words = 500,\r\n  # Define a color\r\n  color = wes_palette(\"Royal1\")\r\n  \r\n)\r\n\r\n\r\n\r\n\r\nAs expected, canada was the most common word across all the paragraphs of text.\r\nGoing back to our task of creating a dataframe out of the corpus, the following code shows how to do that (thanks to this answer on stackoverflow):\r\n\r\n\r\nsuppressPackageStartupMessages(library(tidyverse))\r\n# Corpus to dataframe\r\ndc_df <- data.frame(text = sapply(dc_corpus, as.character), \r\n           stringsAsFactors = FALSE, row.names = NULL) %>% \r\n         as_tibble()\r\n\r\nhead(dc_df, 3)\r\n\r\n\r\n# A tibble: 3 x 1\r\n  text                                                                \r\n  <chr>                                                               \r\n1 \"             STUDY GUIDE\\n           Discover Canada\\nThe Rights a~\r\n2 \"     2\\n\\t\\n  \\n\"                                                  \r\n3 \"The Oath of Citizenship\\nI swear (or affirm)\\nThat I will be faith~\r\n\r\nStep 2: Split the text paragraph in each row into sentences\r\nI used the tidytext package for this step:\r\n\r\n\r\nsuppressPackageStartupMessages(library(tidytext))\r\n## divide into sentences\r\ndc_df_sent <- dc_df %>% \r\n  unnest_tokens(output = sentence, input = text, token = \"sentences\")\r\n\r\nhead(dc_df_sent, 3)\r\n\r\n\r\n# A tibble: 3 x 1\r\n  sentence                                                            \r\n  <chr>                                                               \r\n1 study guide            discover canada the rights and responsibilit~\r\n2 2                                                                   \r\n3 the oath of citizenship i swear (or affirm) that i will be faithful~\r\n\r\nStep 3: Find the sentences containing years\r\nI found all those sentences that contained a four-digit number that indicated a year in the text. Then I filtered out all other sentences and sorted the dataframe by year:\r\n\r\n\r\n## Find which sentence has a 4 digit number\r\ndc_df_year <- dc_df_sent %>% \r\n  mutate(\r\n    has_a_num = str_detect(string = sentence, pattern = \"[[:digit:]]{4}\")\r\n  ) %>% \r\n  filter(has_a_num == TRUE) %>% \r\n  mutate(\r\n    year = str_extract(string = sentence, pattern = \"[[:digit:]]{4}\") %>% \r\n      as.numeric()\r\n  ) %>% \r\n  select(year, sentence) %>% \r\n  arrange(year) \r\n\r\n\r\nhead(dc_df_year, 2)\r\n\r\n\r\n# A tibble: 2 x 2\r\n   year sentence                                                      \r\n  <dbl> <chr>                                                         \r\n1  1215 together, these secure for canadians an 800-year old traditio~\r\n2  1497 picture: (top) indian encampment, fur trade era picture: (rig~\r\n\r\nStep 4: Create a table of year and relevant history:\r\nFinally, I used the reactable package to create an interactive table that contained year as a group. Clicking on a year group reveals history in one or more sentences from Discover Canada:\r\n\r\n\r\nShow code\r\n\r\nsuppressPackageStartupMessages(library(reactable))\r\n\r\nreactable(\r\n  dc_df_year,\r\n  groupBy = \"year\",\r\n  searchable = TRUE,\r\n  filterable = TRUE,\r\n  resizable = TRUE,\r\n  onClick = \"expand\",\r\n  showPageSizeOptions = TRUE,\r\n  columns = list(\r\n    year = colDef(name = \"Year\", maxWidth = 250),\r\n    sentence = colDef( name = \"What happened?\")\r\n   ),\r\n  theme = reactableTheme(backgroundColor = \"#eadbcb\")\r\n)\r\n\r\n\r\n\r\n\r\nI am now ready to delve into the Discover Canada study guide further. Ideally, I want to extract the most useful phrases from each sentence for a given year, but at this point I do not know the best way to do that programmatically. If you have any ideas, please share with me on twitter.\r\n\r\n\r\n\r\n",
    "preview": "https://media.giphy.com/media/3FmmhJdHN4PSESllzZ/giphy.gif",
    "last_modified": "2021-02-07T19:21:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-25-elevation-maps-in-r/",
    "title": "Elevation Maps in R",
    "description": "Learn to create 3D and ridgeline plots with an animation bonus.",
    "author": [
      {
        "name": "Umair Durrani",
        "url": {}
      }
    ],
    "date": "2020-12-25",
    "categories": [],
    "contents": "\r\nIn this post I show you how to create aesthetically pleasant and useful maps in R using ground elevation data.\r\nGetting the Elevation Data in R\r\nI got the elevation data from Geohub which provides the data for Ontario, Canada in the .tif format. The .tif file type can be easily imported in R using the raster package. If you want to use United States data, the elevatr R package can help with that.\r\nI have downloaded 2 files for Ontario, one each for north and south Ontario, and then unzipped them. Let’s start by loading the libraries and importing the data.\r\n\r\n\r\n# Load Libraries ----------------------------------------------------------\r\n\r\nsuppressPackageStartupMessages( library(raster) )\r\nsuppressPackageStartupMessages( library(tidyverse) )\r\nsuppressPackageStartupMessages( library(ggridges) )\r\nsuppressPackageStartupMessages( library(rayshader) )\r\nsuppressPackageStartupMessages( library(gganimate) )\r\nsuppressPackageStartupMessages( library(sf) )\r\n\r\n\r\n\r\n\r\n# Load Data ---------------------------------------------------------------\r\nont_s <- raster(here::here(\"PDEM_South.tif\"))\r\nont_n <- raster(here::here(\"PDEM_North.tif\"))\r\n\r\n\r\n\r\nNote that I used here::here() to get my file paths. Let’s see what’s in these 2 files:\r\n\r\n\r\nont_s\r\n\r\n\r\nclass      : RasterLayer \r\ndimensions : 32001, 55685, 1781975685  (nrow, ncol, ncell)\r\nresolution : 30, 30  (x, y)\r\nextent     : 151020, 1821570, 11620380, 12580410  (xmin, xmax, ymin, ymax)\r\ncrs        : +proj=lcc +lat_0=0 +lon_0=-85 +lat_1=44.5 +lat_2=53.5 +x_0=930000 +y_0=6430000 +datum=NAD83 +units=m +no_defs \r\nsource     : E:/OneDrive - University of Windsor/udurrani_distill/PDEM_South.tif \r\nnames      : PDEM_South \r\nvalues     : 40.00449, 669.0026  (min, max)\r\n\r\nont_n\r\n\r\n\r\nclass      : RasterLayer \r\ndimensions : 29768, 42540, 1266330720  (nrow, ncol, ncell)\r\nresolution : 30, 30  (x, y)\r\nextent     : 160260, 1436460, 12487470, 13380510  (xmin, xmax, ymin, ymax)\r\ncrs        : +proj=lcc +lat_0=0 +lon_0=-85 +lat_1=44.5 +lat_2=53.5 +x_0=930000 +y_0=6430000 +datum=NAD83 +units=m +no_defs \r\nsource     : E:/OneDrive - University of Windsor/udurrani_distill/PDEM_North.tif \r\nnames      : PDEM_North \r\nvalues     : -2.472506, 520.3692  (min, max)\r\n\r\nThey are clearly raster file types. You can see in the dimensions section that the number of cells (i.e. number of data points) is quite a large number for both layers. This means that if we try to plot these data with a typical RAM size, say 8GB, we’d have a bad time1. So, to create maps we’d need to decrease the size of these raster layers.\r\nBefore going to the next step, an important thing to note here is that the crs section above indicates that the units of the x, y and elevation coordinates are meters. The elevation is stored here under the names PDEM_South and PDEM_North in the two raster layers.\r\nSlimming Down the Raster Data\r\nThe sampleRegular function from the raster package lets you take a sample of the original raster data by specifying the size argument. I experimented with different sizes, and was satisfied with size=50000, as it provided a good visual quality in the maps. The last argument here is asRaster that returns a raster data when TRUE.\r\nOnce we get a reduced raster data, we use the rayshader::raster_to_matrix() function to put all the elevation values in a 2D matrix where the 2 dimensions are x and y coordinates. Following shows the results (only the first 5 rows and columns for each layer):\r\n\r\n\r\nont_s_m <- ont_s %>% \r\n  raster::sampleRegular(ont_s, size=50000, asRaster=TRUE) %>%\r\n  rayshader::raster_to_matrix()\r\n\r\nont_s_m[1:5, 1:5]\r\n\r\n\r\n     [,1] [,2]     [,3]     [,4]     [,5]\r\n[1,]   NA   NA       NA 349.9258 337.7366\r\n[2,]   NA   NA 340.8874 345.7653 342.1966\r\n[3,]   NA   NA 350.0495 349.4100 345.2380\r\n[4,]   NA   NA 357.0622 351.3358 331.3604\r\n[5,]   NA   NA 359.2308 363.4488 333.8035\r\n\r\nont_n_m <- ont_n %>% \r\n  raster::sampleRegular(ont_n, size=50000, asRaster=TRUE) %>%\r\n  rayshader::raster_to_matrix()\r\n\r\nont_n_m[1:5, 1:5]\r\n\r\n\r\n     [,1] [,2] [,3] [,4] [,5]\r\n[1,]   NA   NA   NA   NA   NA\r\n[2,]   NA   NA   NA   NA   NA\r\n[3,]   NA   NA   NA   NA   NA\r\n[4,]   NA   NA   NA   NA   NA\r\n[5,]   NA   NA   NA   NA   NA\r\n\r\n3D Raster Maps with Rayshader\r\nWe use the awesome rayshader package here to plot the raster map in 3D.\r\n\r\n\r\nont_n_m %>%\r\n  sphere_shade(texture = \"imhof1\") %>%\r\n  add_shadow(ray_shade(ont_n_m, zscale = 30), 0.5) %>%\r\n  add_shadow(ambient_shade(ont_n_m), 0) %>%\r\n  plot_3d(ont_n_m, zscale = 10, fov = 0, \r\n          theta = 0, zoom = 0.75, phi = 70, \r\n          windowsize = c(1000, 800),\r\n          solid=FALSE)\r\nSys.sleep(0.3)\r\nrender_snapshot(clear=T, title_text = \"Ontario (North)\")\r\n\r\n\r\n\r\nrgl::rgl.close()\r\n\r\n\r\n\r\n\r\nCreating Ridgeline Maps\r\nRaster maps are beautiful but take long time to render. Another technique that can show the elevation data very effectively is a ridgeline plot. I was previously familiar with the ggridges package but was recently reminded of it when I saw the ridgeline plots app made by Andrei Kashcha. So, I decided to use elevation height in the ggridges::geom_ridgeline() function.\r\nBut a ridgeline plot uses 3 arguments, x, y, and height. So, to proceed, we need to extract these coordinates from the raster files. Again, we use the raster::sampleRegular function for slimming the data, but also use the xy=TRUE option to get the three coordinates:\r\n\r\n\r\n## Sample 10000 values\r\ndf_s <- data.frame(sampleRegular(ont_s, 10000, xy=TRUE))\r\ndf_n <- data.frame(sampleRegular(ont_n, 10000, xy=TRUE))\r\n\r\n\r\n## Rename to 'elevation'\r\ndf_s <- df_s %>% \r\n  rename(elevation = PDEM_South)\r\n\r\ndf_n <- df_n %>% \r\n  rename(elevation = PDEM_North)\r\n\r\n\r\n## Combine the two\r\ndf <- bind_rows(df_s, df_n)\r\n\r\nhead(df)\r\n\r\n\r\n       x        y elevation\r\n1 157395 12574035        NA\r\n2 170145 12574035        NA\r\n3 182895 12574035        NA\r\n4 195645 12574035        NA\r\n5 208395 12574035        NA\r\n6 221145 12574035        NA\r\n\r\nNote that elevation above is not completely NA (you are seeing the first 6 rows only).\r\n\r\nMy attempts to use ggridges::geom_ridgeline() were not successful. Instead, the other function, ggridges::geom_density_ridges is what can effectively generate the nice elevation lines for creating the 3D effect that Andrei showed. I learnt this from Travis M. White’s blogpost. I also found great tips for theming the map.\r\n\r\nPlots\r\nThe data is ready now, back to making the maps. A first try:\r\n\r\n\r\nggplot() +\r\n  geom_density_ridges(data = df,\r\n                      aes(x, y, \r\n                          group=y,\r\n                          height = elevation),\r\n                      stat = \"identity\",\r\n                      scale=20) +\r\n  theme_void() \r\n\r\n\r\n\r\n\r\nSee the 3D effect?\r\nThe scale argument controls the overlap between the ridgelines. I experimented with different values and settled with 20.\r\nNow with a dark theme:\r\n\r\n\r\nggplot() +\r\n  geom_density_ridges(data = df,\r\n                      aes(x, y, \r\n                          group=y,\r\n                          height = elevation),\r\n                      stat = \"identity\",\r\n                      scale=20,\r\n                      fill=\"black\",\r\n                      color=\"white\") +\r\n  scale_x_continuous(name = \"Ontario\") +\r\n  theme_void() +\r\n  theme(\r\n             panel.background = element_rect(fill = \"black\"),\r\n            \r\n             plot.background = element_rect(fill = \"black\"),\r\n           \r\n             axis.title.x = element_text(colour = 'white', \r\n                                         size = 18))\r\n\r\n\r\n\r\n\r\nFor some reason, this reminded me of neon signs. So, I got an idea!\r\nAnimating the Ridgeline Map\r\nLet’s animate the map with colors. For doing so, I create 2 more copies of the data df and then give one color to each (there may be a better way to do this). Then I use transition_reveal to color the lines turn by turn:\r\n\r\n\r\ncolos = rep(c(\"red\", \"yellow\", \"green\"), each = nrow(df))\r\n\r\nretro <- bind_rows(df, df, df) %>% \r\n  mutate(colorz = colos)\r\n\r\nani1 <- ggplot(data = retro ,\r\n       aes(x, y, \r\n           group=y,\r\n           height = elevation))+\r\n  geom_density_ridges(stat = \"identity\",\r\n                      scale=20,\r\n                      fill=\"black\",\r\n                      aes(color=colorz)) +\r\n  scale_x_continuous(name = \"Ontario\") +\r\n  theme_void() +\r\n  theme(\r\n    legend.position = \"none\",\r\n    panel.background = element_rect(fill = \"black\"),\r\n    \r\n    plot.background = element_rect(fill = \"black\"),\r\n    \r\n    axis.title.x = element_text(colour = 'white', \r\n                                size = 18)) +\r\n    transition_states(colorz,\r\n                    transition_length = 2,\r\n                    state_length = 1)\r\n\r\nani1\r\n\r\n\r\n\r\n\r\nOf course, we can also animate by x or y dimensions:\r\n\r\n\r\nani2 <- ggplot(data = df,\r\n       aes(x, y, \r\n           group=y,\r\n           height = elevation))+\r\n  geom_density_ridges(stat = \"identity\",\r\n                      scale=8,\r\n                      fill=\"black\",\r\n                      color = \"white\") +\r\n  scale_x_continuous(name = \"Ontario\") +\r\n  theme_void() +\r\n  theme(\r\n    legend.position = \"none\",\r\n    panel.background = element_rect(fill = \"black\"),\r\n    \r\n    plot.background = element_rect(fill = \"black\"),\r\n    \r\n    axis.title.x = element_text(colour = 'white', \r\n                                size = 18)) + \r\n  transition_manual(x, cumulative = T) + \r\n  ease_aes('linear')\r\n\r\n\r\nani2\r\n\r\n\r\n\r\n\r\nThis ends the blogpost. I hope you’d find something useful here for your mapping needs.\r\n\r\nRead the discussion here: https://stackoverflow.com/questions/61535383/r-runs-out-of-memory-plotting-data-frame-with-ggplot2↩︎\r\n",
    "preview": "posts/2020-12-25-elevation-maps-in-r/ontario_map.gif",
    "last_modified": "2021-02-07T16:04:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-19-hellowww/",
    "title": "Hello World",
    "description": "First post",
    "author": [
      {
        "name": "Umair Durrani",
        "url": {}
      }
    ],
    "date": "2020-12-19",
    "categories": [],
    "contents": "\r\nThis is my first post on this new website, created with the distill package in R.\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://media.giphy.com/media/MeJgB3yMMwIaHmKD4z/giphy.gif",
    "last_modified": "2021-02-07T16:04:45-05:00",
    "input_file": {}
  }
]
